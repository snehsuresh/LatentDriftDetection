# ğŸ“ Latent Concept Drift Detection for LLMs & Vision Models

**Detect semantic and distributional drift in both text (LLMs) and images (CLIP) using embedding alignment, Wasserstein distance, and BERTScore.**  
Use it to detect when LLMs' or vision models' "understanding" shifts over time.

---
# ğŸ“Œ Why This Matters

- **LLMs go stale** â€“ A model trained in 2020 won't recognize 2024 slang, trends, or new concepts.
- **Image models experience visual drift** â€“ Camera quality, lighting conditions, and domain changes affect embeddings.
- **Companies need monitoring systems** â€“ Instead of retraining every week, we track **when drift happens** and retrain **only when necessary**.

---
# ğŸ—ï¸ How It Works

âœ… **Text Embedding Drift** â€“ Tracks semantic shifts in your laguage model's embeddings over time.  
âœ… **Image Embedding Drift** â€“ Detects when vision models start seeing the world differently.  
âœ… **Metrics Used:**
   - **BERTScore** â€“ Semantic similarity loss (text-only)
   - **Wasserstein Distance** â€“ How much embedding distributions shift
   - **Alignment Loss** â€“ Geometric shift in embedding space

---
# ğŸ“Š Results

## 1ï¸âƒ£ Simulated Drift (Artificial Noise)

| Metric                   | Text Drift (Simulated) | Image Drift (Simulated) |
|-----------------|----------------|----------------|
| **BERTScore F1** (Semantic Similarity) | `0.7854` | N/A |
| **Wasserstein Distance** (Distributional Shift) | `0.3297` | `0.0672` |
| **Alignment Loss** (Embedding Movement) | `624.55` | `52.47` |

### **ğŸ” Simulated Drift Visualizations**
**Baseline Text Embeddings**  
![Baseline Text](plots/baseline_text_embeddings.png)  

**Drifted Text Embeddings** (after noise injection)  
![Drifted Text](plots/drifted_text_embeddings.png)  

**Baseline Image Embeddings**  
![Baseline Image](plots/baseline_image_embeddings.png)  

**Drifted Image Embeddings** (after simulated jitter)  
![Drifted Image](plots/drifted_image_embeddings.png)  

---
## 2ï¸âƒ£ Real-World Drift (Text & Image)

We compared embeddings from **two different time periods/domains:**
- **Baseline Text:** Wikipedia-style data (wikitext-2)
- **Drift Text:** News articles (AG News)
- **Baseline Images:** Standard CIFAR10 images
- **Drift Images:** CIFAR10 with color distortions (mimicking real-world visual changes)

### **ğŸ“Š Real-World Drift Logs**

| Metric                   | Text Drift (Real) | Image Drift (Real) |
|-----------------|----------------|----------------|
| **BERTScore F1** (Semantic Similarity) | `0.5608` â¬‡ (huge shift) | N/A |
| **Wasserstein Distance** (Distributional Shift) | `0.6268` â¬† (biggest shift) | `0.0784` |
| **Alignment Loss** (Embedding Movement) | `1114.00` â¬† (LLM structure changed) | `80.43` |

### **ğŸ” Real-World Drift Visualizations**
**Baseline Text Embeddings (Wiki-style)**  
![Baseline Text](plots/baseline_text_embeddings.png)  

**Drifted Text Embeddings (News-style)**  
![Drifted Text](plots/drifted_text_embeddings.png)  

**Baseline Image Embeddings**  
![Baseline Image](plots/baseline_image_embeddings.png)  

**Drifted Image Embeddings (Color shift applied)**  
![Drifted Image](plots/drifted_image_embeddings.png)  

---
# âš™ï¸ Installation & Usage

## 1ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
python src/main.py
```

# ğŸš€ Key Takeaways

- Text drift (semantic shift) is significant between domains (Wikipedia â†’ News).
- Image drift (visual distribution shift) is weaker unless major changes (e.g., resolution, dataset shift) occur.
- BERTScore is crucial for text monitoring but doesnâ€™t apply to images.
- Embedding-based methods allow model monitoring without full inference.
